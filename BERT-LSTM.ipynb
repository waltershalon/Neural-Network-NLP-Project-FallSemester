{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e672cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import AutoTokenizer, TFBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "# Load the tokenizer from DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(examples):\n",
    "    \"\"\"Converts the text of each example to a sequence of integers\n",
    "    representing token ids.\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=64,\n",
    "                     padding=\"max_length\")\n",
    "\n",
    "def train(model_path=\"model\", train_path=\"train.csv\", dev_path=\"dev.csv\"):\n",
    "    # Load the CSVs into Huggingface datasets to allow use of the tokenizer\n",
    "    hf_dataset = datasets.load_dataset(\"csv\", data_files={\n",
    "        \"train\": train_path, \"validation\": dev_path})\n",
    "\n",
    "    # The labels are the names of all columns except the first\n",
    "    labels = hf_dataset[\"train\"].column_names[1:]\n",
    "\n",
    "    def gather_labels(example):\n",
    "        \"\"\"Converts the label columns into a list of 0s and 1s\"\"\"\n",
    "        return {\"labels\": [float(example[l]) for l in labels]}\n",
    "\n",
    "    # Convert text and labels to format expected by model\n",
    "    hf_dataset = hf_dataset.map(gather_labels)\n",
    "    hf_dataset = hf_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    # Convert Huggingface datasets to Tensorflow datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': hf_dataset['train']['input_ids'],\n",
    "            'attention_mask': hf_dataset['train']['attention_mask']\n",
    "        },\n",
    "        hf_dataset['train']['labels']\n",
    "    )).batch(32)\n",
    "\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': hf_dataset['validation']['input_ids'],\n",
    "            'attention_mask': hf_dataset['validation']['attention_mask']\n",
    "        },\n",
    "        hf_dataset['validation']['labels']\n",
    "    )).batch(32)\n",
    "\n",
    "    # Define the model architecture\n",
    "    input_ids = tf.keras.layers.Input(shape=(64,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(64,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "    lstm_layer = Bidirectional(LSTM(64, dropout=0.3))(bert_output)\n",
    "    output_layer = Dense(len(labels), activation='sigmoid')(lstm_layer)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n",
    "\n",
    "    # Compile the model with hyperparameters\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=2e-5)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    metric = tf.keras.metrics.F1Score(average=\"micro\", threshold=0.5)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    # Fit the model to the training data, monitoring performance on the dev data\n",
    "    model.fit(train_dataset, epochs=3, validation_data=dev_dataset)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad34e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_path=\"model\", input_path=\"dev.csv\"):\n",
    "    # Load the saved model\n",
    "    model = tf.keras.model.load_model(model_path)\n",
    "    \n",
    "    # Load the data for prediction\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # Create input features in the same way as in train()\n",
    "    hf_dataset = datasets.Dataset.from_pandas(df)\n",
    "    hf_dataset = hf_dataset.map(tokenize, batched=True)\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "        'input_ids': hf_dataset['input_ids'],\n",
    "        'attention_mask': hf_dataset['attention_mask']\n",
    "    }).batch(16)\n",
    "\n",
    "    # Generate predictions from model\n",
    "    predictions = model.predict(tf_dataset).logits\n",
    "    predictions = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "    # Assign predictions to label columns in Pandas data frame\n",
    "    df.iloc[:, 1:] = predictions\n",
    "\n",
    "    # Write the Pandas dataframe to a zipped CSV file\n",
    "    df.to_csv(\"submission.zip\", index=False, compression=dict(\n",
    "        method='zip', archive_name='submission.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dae395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/shalonwalter/.cache/huggingface/datasets/csv/default-a9becbf2fe769e87/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db79bd628514ef4b4c49fccf3c282a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/shalonwalter/.cache/huggingface/datasets/csv/default-a9becbf2fe769e87/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-185ab8c5b3aca745.arrow\n",
      "Loading cached processed dataset at /Users/shalonwalter/.cache/huggingface/datasets/csv/default-a9becbf2fe769e87/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fe1b49971e88ac39.arrow\n",
      "Loading cached processed dataset at /Users/shalonwalter/.cache/huggingface/datasets/csv/default-a9becbf2fe769e87/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-3ebdde82a18b2009.arrow\n",
      "Loading cached processed dataset at /Users/shalonwalter/.cache/huggingface/datasets/csv/default-a9becbf2fe769e87/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-c6d229521faeeb55.arrow\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "788/788 [==============================] - 1248s 2s/step - loss: 0.1071 - f1_score: 0.7436 - val_loss: 0.0682 - val_f1_score: 0.8406\n",
      "Epoch 2/3\n",
      "788/788 [==============================] - 1242s 2s/step - loss: 0.0646 - f1_score: 0.8449 - val_loss: 0.0632 - val_f1_score: 0.8436\n",
      "Epoch 3/3\n",
      "788/788 [==============================] - 1243s 2s/step - loss: 0.0521 - f1_score: 0.8794 - val_loss: 0.0649 - val_f1_score: 0.8444\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x14e0d3110>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x14c96d090>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x14e209b10>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x14c96c610>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x14e03ced0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x149797290>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "train(model_path=\"model\", train_path=\"train.csv\", dev_path=\"dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5757a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model_path=\"model\", input_path=\"dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a44a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
